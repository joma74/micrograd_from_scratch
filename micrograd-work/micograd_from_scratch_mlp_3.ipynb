{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7932766b-9a8d-466c-b779-916ae5064ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.wrighters.io/using-autoreload-to-speed-up-ipython-and-jupyter-work/\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# micograd_from_scratch_mlp_1, but by applying the modularized Classes from micrograd.nn\n",
    "from micrograd.tracegraph import draw_dot\n",
    "from micrograd.topo import build, findLeafNodes\n",
    "from micrograd.nn import Neuron, Layer, MLP\n",
    "from micrograd.engine import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e9a6196-ceb5-4a22-8bb0-29111ea24a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.1679740798498234, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=0.925065047449031, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=0.849984319542622, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=-0.10484121672176865, grad=0, op=tanh, label=L2|N0|o)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset with 4 Input values, assigned each to each of the 3 Neurons of Layer 0.\n",
    "# The rows in the first column are the Input values assigned to each Neuron 0, 1, 2 and 3 of Layer 0.\n",
    "# The rows in the second column are the Input values assigned to each Neuron 0, 1, 2 and 3 of Layer 0.\n",
    "# The rows in the third column are the Input values assigned to each Neuron 0, 1, 2 and 3 of Layer 0.\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0], # Example #0 \n",
    "    [3.0, -1.0, 0.5], # Example #1\n",
    "    [0.5, 1.0, 1.0],  # Example #2\n",
    "    [1.0, 1.0, -1.0], # Example #3\n",
    "]\n",
    "# Desired targets for each Example #; a simple binary classifier; Also called the g(round) t(ruths)\n",
    "# Values labeled gt0, gt1, gt2 and gt3 are the ground truths for each Example\n",
    "ygts = [Value(1.0, _label='gt0'), Value(-1.0, _label='gt1'), Value(-1.0, _label='gt2'), Value(1.0, _label='gt3')]\n",
    "# A MLP neuronal network with 3 Input value and, 3 Layers by 4x4x1 Neurons\n",
    "mlp_nn1 = MLP(3, [4, 4, 1])\n",
    "# WANT\n",
    "# The current prediction for each Example\n",
    "#   The MLP to output  1.0 given Example #0\n",
    "#   The MLP to output -1.0 given Example #1\n",
    "#   The MLP to output -1.0 given Example #2\n",
    "#   The MLP to output  1.0 given Example #3\n",
    "yspred = [(mlp_nn1(x)) for x in xs]\n",
    "# Values labeled L2 | N0 | o are the ys predictions for each Example\n",
    "yspred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16854ec8-5f93-4981-ae17-b8b870abfa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for all ground truths: Value(data=9.713154984831798, grad=0, op=+, label=losssum)\n"
     ]
    }
   ],
   "source": [
    "# WANT\n",
    "# How do we tune the weights to better predict the desired targets?\n",
    "# Calculate a single number that measures the total performance of the neural net.\n",
    "# This single number is called the Loss. \n",
    "# So first, we implement the Loss function with a mean squared error Loss. The Loss is the difference between the \n",
    "# prediction and the gound truth of y. Squared, to always get a positive number.\n",
    "losss = [(ypred - ygt)**2 for ygt, ypred in zip(ygts, yspred)]\n",
    "# label the loss by index for each loss\n",
    "for idx, loss in enumerate(losss): loss._label = f'loss{idx}'\n",
    "# The overall loss is\n",
    "losssum = sum(losss)\n",
    "losssum._label = 'losssum'\n",
    "print(f'loss for all ground truths: {losssum}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef238017-14bd-4158-9adb-4b4b2304d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_dot of losssum before calling backward -> see doc/micograd_from_scratch_mlp_3_withlosssum_beforebackward.svg\n",
    "# draw_dot(losssum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2117e6-fc07-4a69-91a6-f06d01f85424",
   "metadata": {},
   "outputs": [],
   "source": [
    "losssum.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e06ea4c3-aaa9-47d4-9d63-95030d97ddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_dot of losssum after calling backward -> see doc/micograd_from_scratch_mlp_3_withlosssum_afterbackward.svg\n",
    "# draw_dot(losssum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cca1ab05-44ff-47d0-9f5f-fa51d5f4292b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we look at the Weight of some Neuron of some Layer: Value(data=0.29611421237095814, grad=-1.4961196133929398, op=prim, label=L0|N0|w0)\n",
      "... at the grad, we see that it's influence is ? positive or negative: -1.4961196133929398\n"
     ]
    }
   ],
   "source": [
    "# If we look at the 1st Weight of the 1st Neuron of the first Layer ...\n",
    "mlp_nn1.layers[0].neurons[0].w[0]\n",
    "# ... at the grad, we see that it's influence is positive\n",
    "print(f'If we look at the Weight of some Neuron of some Layer: {mlp_nn1.layers[0].neurons[0].w[0]}')\n",
    "print(f'... at the grad, we see that it\\'s influence is ? positive or negative: {mlp_nn1.layers[0].neurons[0].w[0].grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3084f71-0668-4e52-b83b-7948426f9ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# If the Grad of this Weight of this particular Neuron of this particular Layer is positive, the influence of the Weight to \n",
    "# the Loss is also positive. So decreasing the Weight of this particular Neuron would make the loss go down.\n",
    "#\n",
    "# If the Grad of this Weight of this particular Neuron of this particular Layer is negative, the influence of the Weight to\n",
    "# the Loss is also negative. So increasing the Weight of this particular Neuron would make the loss go down.\n",
    "#\n",
    "# WANT\n",
    "# A convenience methode, to gather all those parameters called Weights and Biases so we can change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c6200f-24df-4d2f-a228-0c0a2f930f70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
