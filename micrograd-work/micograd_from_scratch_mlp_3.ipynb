{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7932766b-9a8d-466c-b779-916ae5064ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# https://www.wrighters.io/using-autoreload-to-speed-up-ipython-and-jupyter-work/\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# micograd_from_scratch_mlp_1, but by applying the modularized Classes from micrograd.nn\n",
    "from micrograd.tracegraph import draw_dot\n",
    "from micrograd.topo import build, findLeafNodes\n",
    "from micrograd.nn import Neuron, Layer, MLP\n",
    "from micrograd.engine import Value\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e9a6196-ceb5-4a22-8bb0-29111ea24a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.8024582077087925, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=0.5413612523015063, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=0.35568112667755925, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=0.7577300512482477, grad=0, op=tanh, label=L2|N0|o)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset with 4 Input values, assigned each to each of the 3 Neurons of Layer 0.\n",
    "# The rows in the first column are the Input values assigned to each Neuron 0, 1, 2 and 3 of Layer 0.\n",
    "# The rows in the second column are the Input values assigned to each Neuron 0, 1, 2 and 3 of Layer 0.\n",
    "# The rows in the third column are the Input values assigned to each Neuron 0, 1, 2 and 3 of Layer 0.\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0], # Example #0 \n",
    "    [3.0, -1.0, 0.5], # Example #1\n",
    "    [0.5, 1.0, 1.0],  # Example #2\n",
    "    [1.0, 1.0, -1.0], # Example #3\n",
    "]\n",
    "# Desired targets for each Example #; a simple binary classifier; Also called the g(round) t(ruths)\n",
    "# Values labeled gt0, gt1, gt2 and gt3 are the ground truths for each Example\n",
    "ygts = [Value(1.0, _label='gt0'), Value(-1.0, _label='gt1'), Value(-1.0, _label='gt2'), Value(1.0, _label='gt3')]\n",
    "# A MLP neuronal network with 3 Input value and, 3 Layers by 4x4x1 Neurons\n",
    "mlp_nn1 = MLP(3, [4, 4, 1])\n",
    "# WANT\n",
    "# The current prediction for each Example\n",
    "#   The MLP to output  1.0 given Example #0\n",
    "#   The MLP to output -1.0 given Example #1\n",
    "#   The MLP to output -1.0 given Example #2\n",
    "#   The MLP to output  1.0 given Example #3\n",
    "yspred = [(mlp_nn1(x)) for x in xs]\n",
    "# Values labeled L2 | N0 | o are the ys predictions for each Example\n",
    "yspred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16854ec8-5f93-4981-ae17-b8b870abfa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for all ground truths: Value(data=4.311383315096003, grad=0, op=+, label=losssum)\n"
     ]
    }
   ],
   "source": [
    "# WANT\n",
    "# How do we tune the weights to better predict the desired targets?\n",
    "# Calculate a single number that measures the total performance of the neural net.\n",
    "# This single number is called the Loss. \n",
    "# So first, we implement the Loss function with a mean squared error Loss. The Loss is the difference between the \n",
    "# prediction and the gound truth of y. Squared, to always get a positive number.\n",
    "losss = [(ypred - ygt)**2 for ygt, ypred in zip(ygts, yspred)]\n",
    "# label the loss by index for each loss\n",
    "for idx, loss in enumerate(losss): loss._label = f'loss{idx}'\n",
    "# The overall loss is\n",
    "losssum = sum(losss)\n",
    "losssum._label = 'losssum'\n",
    "print(f'loss for all ground truths: {losssum}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef238017-14bd-4158-9adb-4b4b2304d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_dot of losssum before calling backward -> see doc/micograd_from_scratch_mlp_3_withlosssum_beforebackward.svg\n",
    "# draw_dot(losssum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2117e6-fc07-4a69-91a6-f06d01f85424",
   "metadata": {},
   "outputs": [],
   "source": [
    "losssum.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e06ea4c3-aaa9-47d4-9d63-95030d97ddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_dot of losssum after calling backward -> see doc/micograd_from_scratch_mlp_3_withlosssum_afterbackward.svg\n",
    "# draw_dot(losssum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cca1ab05-44ff-47d0-9f5f-fa51d5f4292b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we look at some weight - here the 1st Weight of the 1st Neuron of the 1st Layer -...\n",
      "Value(data=-0.959487738115776, grad=-0.24558073915563292, op=prim, label=L0|N0|w0)\n",
      "... escpecially at its's data ...\n",
      "-0.959487738115776\n",
      "... and at it's grad ...\n",
      "-0.24558073915563292\n",
      "... we see that the grad's influence is ? positive or ? negative\n",
      "\n",
      "If the Grad of this Weight of this particular Neuron of this particular Layer is positive, the influence of the Weight to \n",
      "the Loss is also positive. So decreasing the Weight of this particular Neuron would make the loss go down.\n",
      "\n",
      "If the Grad of this Weight of this particular Neuron of this particular Layer is negative, the influence of the Weight to\n",
      "the Loss is also negative. So increasing the Weight of this particular Neuron would make the loss go down.\n"
     ]
    }
   ],
   "source": [
    "# If we look at some weight - here the 1st Weight of the 1st Neuron of the 1st Layer - ...\n",
    "print(f'If we look at some weight - here the 1st Weight of the 1st Neuron of the 1st Layer -...\\n{mlp_nn1.layers[0].neurons[0].w[0]}')\n",
    "# ... escpecially at its's data ...\n",
    "print(f'... escpecially at its\\'s data ...\\n{mlp_nn1.layers[0].neurons[0].w[0].data}')\n",
    "# ... and at its's grad\n",
    "print(f'... and at it\\'s grad ...\\n{mlp_nn1.layers[0].neurons[0].w[0].grad}')\n",
    "print(f'... we see that the grad\\'s influence is ? positive or ? negative')\n",
    "#\n",
    "print(\"\")\n",
    "print(\"If the Grad of this Weight of this particular Neuron of this particular Layer is positive, the influence of the Weight to \")\n",
    "print(\"the Loss is also positive. So decreasing the Weight of this particular Neuron would make the loss go down.\")\n",
    "print(\"\")\n",
    "print(\"If the Grad of this Weight of this particular Neuron of this particular Layer is negative, the influence of the Weight to\")\n",
    "print(\"the Loss is also negative. So increasing the Weight of this particular Neuron would make the loss go down.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3084f71-0668-4e52-b83b-7948426f9ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters of the MLP:\n",
      "Value(data=-0.959487738115776, grad=-0.24558073915563292, op=prim, label=L0|N0|w0)\n",
      "Value(data=0.375987038868441, grad=-0.47746037458081275, op=prim, label=L0|N0|w1)\n",
      "Value(data=-0.41034046594409146, grad=-0.4603471973615333, op=prim, label=L0|N0|w2)\n",
      "Value(data=-0.5310559751256543, grad=-0.47132442367378147, op=prim, label=L0|N0|b)\n",
      "Value(data=-0.2587547667948096, grad=-11.328698606799321, op=prim, label=L0|N1|w0)\n",
      "Value(data=0.0403237811106445, grad=2.849281943171405, op=prim, label=L0|N1|w1)\n",
      "Value(data=0.5833304446497909, grad=-3.553646227793297, op=prim, label=L0|N1|w2)\n",
      "Value(data=0.4031334249509986, grad=-4.752577074474798, op=prim, label=L0|N1|b)\n",
      "Value(data=-0.08045905136012599, grad=0.5985595195990767, op=prim, label=L0|N2|w0)\n",
      "Value(data=0.8449829371178346, grad=-0.18795489475913127, op=prim, label=L0|N2|w1)\n",
      "Value(data=0.08749341362998475, grad=0.10041199251109707, op=prim, label=L0|N2|w2)\n",
      "Value(data=0.3596692071246599, grad=0.20629667644175093, op=prim, label=L0|N2|b)\n",
      "Value(data=0.7796002006479592, grad=-1.2415173401901167, op=prim, label=L0|N3|w0)\n",
      "Value(data=0.04629929934958121, grad=-1.5842420460481015, op=prim, label=L0|N3|w1)\n",
      "Value(data=-0.6117195051213022, grad=-1.8156334008752415, op=prim, label=L0|N3|w2)\n",
      "Value(data=-0.22379499140156267, grad=-1.8490957812499142, op=prim, label=L0|N3|b)\n",
      "Value(data=0.5919572162407445, grad=2.6655881355656974, op=prim, label=L1|N0|w0)\n",
      "Value(data=0.7502516922776772, grad=-0.7222992329664458, op=prim, label=L1|N0|w1)\n",
      "Value(data=-0.6534884560395511, grad=0.019621007775612112, op=prim, label=L1|N0|w2)\n",
      "Value(data=0.15380220502080544, grad=-1.0893193840900592, op=prim, label=L1|N0|w3)\n",
      "Value(data=-0.2328772444323277, grad=-2.9175329924542477, op=prim, label=L1|N0|b)\n",
      "Value(data=-0.12066568983712389, grad=-2.7278817393329047, op=prim, label=L1|N1|w0)\n",
      "Value(data=-0.8987506612414735, grad=1.1114592993813779, op=prim, label=L1|N1|w1)\n",
      "Value(data=-0.1225969681040342, grad=0.28475382209427597, op=prim, label=L1|N1|w2)\n",
      "Value(data=-0.7520032732538868, grad=0.6381276471785513, op=prim, label=L1|N1|w3)\n",
      "Value(data=0.6533380741557036, grad=3.016088851186503, op=prim, label=L1|N1|b)\n",
      "Value(data=-0.618663543720062, grad=1.054383458376593, op=prim, label=L1|N2|w0)\n",
      "Value(data=0.6506394867425636, grad=-0.028228791546522865, op=prim, label=L1|N2|w1)\n",
      "Value(data=-0.019294303619492137, grad=0.6815316267310784, op=prim, label=L1|N2|w2)\n",
      "Value(data=-0.9494788848380327, grad=-0.8160741463350675, op=prim, label=L1|N2|w3)\n",
      "Value(data=0.537267865322975, grad=-1.0223003498168848, op=prim, label=L1|N2|b)\n",
      "Value(data=-0.3284652770631633, grad=-1.7297032682090074, op=prim, label=L1|N3|w0)\n",
      "Value(data=-0.32139556354765086, grad=0.5823595342024817, op=prim, label=L1|N3|w1)\n",
      "Value(data=-0.6331299279255371, grad=0.13549746095349718, op=prim, label=L1|N3|w2)\n",
      "Value(data=-0.7200097414668492, grad=0.5547449138344112, op=prim, label=L1|N3|w3)\n",
      "Value(data=-0.20094822814925162, grad=1.9157080532314734, op=prim, label=L1|N3|b)\n",
      "Value(data=-0.9164493483231038, grad=-2.025157537341182, op=prim, label=L2|N0|w0)\n",
      "Value(data=0.7677588618193489, grad=1.0702685782554813, op=prim, label=L2|N0|w1)\n",
      "Value(data=-0.510885440797914, grad=2.7343297817332375, op=prim, label=L2|N0|w2)\n",
      "Value(data=0.48313247676857607, grad=-1.005128353278395, op=prim, label=L2|N0|w3)\n",
      "Value(data=0.24915115041220282, grad=4.200600428883557, op=prim, label=L2|N0|b)\n",
      "\n",
      "Number of Parameters of the MLP: 41\n"
     ]
    }
   ],
   "source": [
    "# WANT\n",
    "# A convenience methode, to gather all those parameters called Weights and Biases so we can change them.\n",
    "# To print each parameter on a line, we use print but instead of to the console, we print to a variable \n",
    "output = StringIO()\n",
    "print(*mlp_nn1.parameters(), sep=\"\\n\", file=output)\n",
    "print(f'Parameters of the MLP:\\n{output.getvalue()}')\n",
    "print(f'Number of Parameters of the MLP: {len(mlp_nn1.parameters())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35c6200f-24df-4d2f-a228-0c0a2f930f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the current mlp the grad of N0 in L0 -\n",
      "-0.24558073915563292\n",
      "- is negative. Therefore the influence of the Weight to the Loss is also negative.\n",
      "So increasing the Weight of this particular Neuron would make the loss go down.\n"
     ]
    }
   ],
   "source": [
    "print(f'In the current mlp the grad of N0 in L0 -\\n{mlp_nn1.layers[0].neurons[0].w[0].grad}')\n",
    "print(f'- is negative. Therefore the influence of the Weight to the Loss is also negative.')\n",
    "print(f'So increasing the Weight of this particular Neuron would make the loss go down.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f9e29d8-5909-4166-9026-1b740bb5a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we go to change the parameter slightly according to the gradient information to minimize the Loss function.\n",
    "# In a gradient descent scheme, we think of the Gradient Descent as a vector pointing in the direction of increased loss.\n",
    "# As we want to decrease the loss, we must go to change the parameter in the opposite direction.\n",
    "# So we increase the data of each parameter by multiplying with the negative 1% of it's grad\n",
    "for p in mlp_nn1.parameters():\n",
    "    p.data += -0.01 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "48c4922e-9546-4b84-b1ff-489d6a6be002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After increasing it, the data is\n",
      "Value(data=-0.9570319307242197, grad=-0.24558073915563292, op=prim, label=L0|N0|w0)\n"
     ]
    }
   ],
   "source": [
    "print(f'After increasing it, the data is\\n{mlp_nn1.layers[0].neurons[0].w[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea523f4e-b0b4-42cf-bc83-b4baa51d3140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we are making a second forward pass ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Value(data=0.6864091663240514, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=-0.33709395290648175, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=0.08774657037385546, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=0.634525942145128, grad=0, op=tanh, label=L2|N0|o)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Now we are making a next forward pass ...\")\n",
    "yspred_1 = [(mlp_nn1(x)) for x in xs]\n",
    "# Values labeled L2 | N0 | o are the ys predictions for each Example\n",
    "yspred_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e363f335-8e25-479d-b834-a0af08aa18a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... and recalculate the loss\n",
      "loss for all ground truths: Value(data=1.8545475265637212, grad=0, op=+, label=losssum)\n"
     ]
    }
   ],
   "source": [
    "print(\"... and recalculate the loss\")\n",
    "losss_1 = [(ypred - ygt)**2 for ygt, ypred in zip(ygts, yspred_1)]\n",
    "# label the loss by index for each loss\n",
    "for idx, loss in enumerate(losss_1): loss._label = f'loss{idx}'\n",
    "# The overall loss is\n",
    "losssum_1 = sum(losss_1)\n",
    "losssum_1._label = 'losssum'\n",
    "print(f'loss for all ground truths: {losssum_1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9b5367fd-b25e-45e2-9250-6b33949b65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we start the next iteration #2 to minimize the loss\n",
    "losssum_1.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9e09f92-6485-4d52-b00a-5a69202ce86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After backward, the data is\n",
      "Value(data=-0.9570319307242197, grad=-0.500757439537063, op=prim, label=L0|N0|w0)\n"
     ]
    }
   ],
   "source": [
    "print(f'After backward#2, the data is\\n{mlp_nn1.layers[0].neurons[0].w[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8791da0b-1dd5-4346-9186-2777991abebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we go to change the parameter slightly according to the gradient information to minimize the Loss function.\n",
    "# In a gradient descent scheme, we think of the Gradient Descent as a vector pointing in the direction of increased loss.\n",
    "# As we want to decrease the loss, we must go to change the parameter in the opposite direction.\n",
    "# So we increase the data of each parameter by multiplying with the negative 1% of it's grad\n",
    "for p in mlp_nn1.parameters():\n",
    "    p.data += -0.01 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3b6bba4b-4bce-41eb-a41f-1a2c56c3ac36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we are making a next forward pass ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Value(data=0.41459615714647247, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=-0.8762625661546329, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=-0.38381458442773747, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=0.3673637661304555, grad=0, op=tanh, label=L2|N0|o)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Now we are making a next forward pass ...\")\n",
    "yspred_2 = [(mlp_nn1(x)) for x in xs]\n",
    "# Values labeled L2 | N0 | o are the ys predictions for each Example\n",
    "yspred_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0822087b-5d3a-40c3-a49e-1d74c93da067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... and recalculate the loss\n",
      "loss for all ground truths: Value(data=1.137921682530917, grad=0, op=+, label=losssum)\n"
     ]
    }
   ],
   "source": [
    "print(\"... and recalculate the loss\")\n",
    "losss_2 = [(ypred - ygt)**2 for ygt, ypred in zip(ygts, yspred_2)]\n",
    "# label the loss by index for each loss\n",
    "for idx, loss in enumerate(losss_2): loss._label = f'loss{idx}'\n",
    "# The overall loss is\n",
    "losssum_2 = sum(losss_2)\n",
    "losssum_2._label = 'losssum'\n",
    "print(f'loss for all ground truths: {losssum_2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "77842481-b56c-47be-8281-61b8a18a525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we start the next iteration #3 to minimize the loss\n",
    "losssum_2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2f27c7ec-23c3-469e-8010-2f6356ff6db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After backward#3, the data is\n",
      "Value(data=-0.9520243563288491, grad=-0.8243632146237716, op=prim, label=L0|N0|w0)\n"
     ]
    }
   ],
   "source": [
    "print(f'After backward#3, the data is\\n{mlp_nn1.layers[0].neurons[0].w[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "68ddcdbd-ea9d-449b-9bd3-4e4774e21205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we go to change the parameter slightly according to the gradient information to minimize the Loss function.\n",
    "# In a gradient descent scheme, we think of the Gradient Descent as a vector pointing in the direction of increased loss.\n",
    "# As we want to decrease the loss, we must go to change the parameter in the opposite direction.\n",
    "# So we increase the data of each parameter by multiplying with the negative 1% of it's grad\n",
    "for p in mlp_nn1.parameters():\n",
    "    p.data += -0.01 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b3ff22c5-408c-4af8-b2fd-3394d279ae9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we are making a next forward pass ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Value(data=0.41984219150271845, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=-0.9478352811859603, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=-0.7026437031622795, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=0.27078194954012846, grad=0, op=tanh, label=L2|N0|o)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Now we are making a next forward pass ...\")\n",
    "yspred_3 = [(mlp_nn1(x)) for x in xs]\n",
    "# Values labeled L2 | N0 | o are the ys predictions for each Example\n",
    "yspred_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "57ef9817-a981-46c4-9218-5caf34f04634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... and recalculate the loss\n",
      "loss for all ground truths: Value(data=0.9594839730348546, grad=0, op=+, label=losssum)\n"
     ]
    }
   ],
   "source": [
    "print(\"... and recalculate the loss\")\n",
    "losss_3 = [(ypred - ygt)**2 for ygt, ypred in zip(ygts, yspred_3)]\n",
    "# label the loss by index for each loss\n",
    "for idx, loss in enumerate(losss_3): loss._label = f'loss{idx}'\n",
    "# The overall loss is\n",
    "losssum_3 = sum(losss_3)\n",
    "losssum_3._label = 'losssum'\n",
    "print(f'loss for all ground truths: {losssum_3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fb8d2ea0-166e-4ede-af02-0522bf44f5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we start the next iteration #4 to minimize the loss\n",
    "losssum_3.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6d5a596a-841f-4796-9cb3-d162125313ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After backward#4, the data is\n",
      "Value(data=-0.9437807241826114, grad=-1.2634181690347575, op=prim, label=L0|N0|w0)\n"
     ]
    }
   ],
   "source": [
    "print(f'After backward#4, the data is\\n{mlp_nn1.layers[0].neurons[0].w[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e29c2ab8-bf72-47a9-b41c-3efd6a737072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we go to change the parameter slightly according to the gradient information to minimize the Loss function.\n",
    "# In a gradient descent scheme, we think of the Gradient Descent as a vector pointing in the direction of increased loss.\n",
    "# As we want to decrease the loss, we must go to change the parameter in the opposite direction.\n",
    "# So we increase the data of each parameter by multiplying with the negative 1% of it's grad\n",
    "for p in mlp_nn1.parameters():\n",
    "    p.data += -0.01 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f48844ca-ff29-4fa5-a749-4d26e137d5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we are making a next forward pass ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Value(data=0.7237366492032441, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=-0.967311577375384, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=-0.8402956602263896, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=0.49660603112648943, grad=0, op=tanh, label=L2|N0|o)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Now we are making a next forward pass ...\")\n",
    "yspred_4 = [(mlp_nn1(x)) for x in xs]\n",
    "# Values labeled L2 | N0 | o are the ys predictions for each Example\n",
    "yspred_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "998a20dc-48df-4098-91ea-db8601c6a410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... and recalculate the loss\n",
      "loss for all ground truths: Value(data=0.3563009360078866, grad=0, op=+, label=losssum)\n"
     ]
    }
   ],
   "source": [
    "print(\"... and recalculate the loss\")\n",
    "losss_4 = [(ypred - ygt)**2 for ygt, ypred in zip(ygts, yspred_4)]\n",
    "# label the loss by index for each loss\n",
    "for idx, loss in enumerate(losss_4): loss._label = f'loss{idx}'\n",
    "# The overall loss is\n",
    "losssum_4 = sum(losss_4)\n",
    "losssum_4._label = 'losssum'\n",
    "print(f'loss for all ground truths: {losssum_4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cc5115cf-79ba-40d2-abcf-d2a83a99fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we start the next iteration #5 to minimize the loss\n",
    "losssum_4.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a500bbb2-bb13-49fd-ad19-6eba32e5387e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After backward#5, the data is\n",
      "Value(data=-0.9311465424922638, grad=-1.4899964043175222, op=prim, label=L0|N0|w0)\n"
     ]
    }
   ],
   "source": [
    "print(f'After backward#5, the data is\\n{mlp_nn1.layers[0].neurons[0].w[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b6b0da7e-7593-40b6-9874-4747afc1b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we go to change the parameter slightly according to the gradient information to minimize the Loss function.\n",
    "# In a gradient descent scheme, we think of the Gradient Descent as a vector pointing in the direction of increased loss.\n",
    "# As we want to decrease the loss, we must go to change the parameter in the opposite direction.\n",
    "# So we increase the data of each parameter by multiplying with the negative 1% of it's grad\n",
    "for p in mlp_nn1.parameters():\n",
    "    p.data += -0.01 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bd58257d-b6dc-4e43-b5e9-22b3aba9f82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we are making a next forward pass ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Value(data=0.8605699469316133, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=-0.9768581419767903, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=-0.9038499001183877, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=0.7235049940253461, grad=0, op=tanh, label=L2|N0|o)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Now we are making a next forward pass ...\")\n",
    "yspred_5 = [(mlp_nn1(x)) for x in xs]\n",
    "# Values labeled L2 | N0 | o are the ys predictions for each Example\n",
    "yspred_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3494e06e-0945-4b3a-855a-075ec6ae9852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... and recalculate the loss\n",
      "loss for all ground truths: Value(data=0.10567061532758747, grad=0, op=+, label=losssum)\n"
     ]
    }
   ],
   "source": [
    "print(\"... and recalculate the loss\")\n",
    "losss_5 = [(ypred - ygt)**2 for ygt, ypred in zip(ygts, yspred_5)]\n",
    "# label the loss by index for each loss\n",
    "for idx, loss in enumerate(losss_5): loss._label = f'loss{idx}'\n",
    "# The overall loss is\n",
    "losssum_5 = sum(losss_5)\n",
    "losssum_5._label = 'losssum'\n",
    "print(f'loss for all ground truths: {losssum_5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4a41f0c4-ea35-44fe-a0d8-56447e00be1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we start the next iteration #6 to minimize the loss\n",
    "losssum_5.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "065d65a4-bb98-4fb7-b2f0-0433c2a73863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After backward#6, the data is\n",
      "Value(data=-0.9162465784490886, grad=-1.566860467143402, op=prim, label=L0|N0|w0)\n"
     ]
    }
   ],
   "source": [
    "print(f'After backward#6, the data is\\n{mlp_nn1.layers[0].neurons[0].w[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6b3a4875-4f3f-4cf0-a0d9-4a7b0580945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we go to change the parameter slightly according to the gradient information to minimize the Loss function.\n",
    "# In a gradient descent scheme, we think of the Gradient Descent as a vector pointing in the direction of increased loss.\n",
    "# As we want to decrease the loss, we must go to change the parameter in the opposite direction.\n",
    "# So we increase the data of each parameter by multiplying with the negative 1% of it's grad\n",
    "for p in mlp_nn1.parameters():\n",
    "    p.data += -0.01 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "36213dab-8974-4e10-b391-6b06b17fa1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we are making a next forward pass ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9105576870819314, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=-0.9827749432042266, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=-0.9353791751927711, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=0.845631470132911, grad=0, op=tanh, label=L2|N0|o)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Now we are making a next forward pass ...\")\n",
    "yspred_6 = [(mlp_nn1(x)) for x in xs]\n",
    "# Values labeled L2 | N0 | o are the ys predictions for each Example\n",
    "yspred_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1e82a1be-6e61-449e-bc98-1853ac9fdc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... and recalculate the loss\n",
      "loss for all ground truths: Value(data=0.036302123933844244, grad=0, op=+, label=losssum)\n"
     ]
    }
   ],
   "source": [
    "print(\"... and recalculate the loss\")\n",
    "losss_6 = [(ypred - ygt)**2 for ygt, ypred in zip(ygts, yspred_6)]\n",
    "# label the loss by index for each loss\n",
    "for idx, loss in enumerate(losss_6): loss._label = f'loss{idx}'\n",
    "# The overall loss is\n",
    "losssum_6 = sum(losss_6)\n",
    "losssum_6._label = 'losssum'\n",
    "print(f'loss for all ground truths: {losssum_6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5655dcad-362d-43fd-b5fc-5ab030f31987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we start the next iteration #7 to minimize the loss\n",
    "losssum_6.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7f297c91-b5ad-4962-b29f-5475a52815e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After backward#7, the data is\n",
      "Value(data=-0.9005779737776546, grad=-1.5926440655553684, op=prim, label=L0|N0|w0)\n"
     ]
    }
   ],
   "source": [
    "print(f'After backward#7, the data is\\n{mlp_nn1.layers[0].neurons[0].w[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2de42da6-84aa-4c00-b330-e2b2f421f01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we go to change the parameter slightly according to the gradient information to minimize the Loss function.\n",
    "# In a gradient descent scheme, we think of the Gradient Descent as a vector pointing in the direction of increased loss.\n",
    "# As we want to decrease the loss, we must go to change the parameter in the opposite direction.\n",
    "# So we increase the data of each parameter by multiplying with the negative 1% of it's grad\n",
    "for p in mlp_nn1.parameters():\n",
    "    p.data += -0.01 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f28fae9a-94cd-4ae2-9fd9-491a65a84f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we are making a next forward pass ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9356801507960734, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=-0.9868019791950361, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=-0.9511034875965612, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=0.9047017212725309, grad=0, op=tanh, label=L2|N0|o)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Now we are making a next forward pass ...\")\n",
    "yspred_7 = [(mlp_nn1(x)) for x in xs]\n",
    "# Values labeled L2 | N0 | o are the ys predictions for each Example\n",
    "yspred_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b35494fc-4434-40a5-bff3-d0312b0b82c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... and recalculate the loss\n",
      "loss for all ground truths: Value(data=0.01578386160842216, grad=0, op=+, label=losssum)\n"
     ]
    }
   ],
   "source": [
    "print(\"... and recalculate the loss\")\n",
    "losss_7 = [(ypred - ygt)**2 for ygt, ypred in zip(ygts, yspred_7)]\n",
    "# label the loss by index for each loss\n",
    "for idx, loss in enumerate(losss_7): loss._label = f'loss{idx}'\n",
    "# The overall loss is\n",
    "losssum_7 = sum(losss_7)\n",
    "losssum_7._label = 'losssum'\n",
    "print(f'loss for all ground truths: {losssum_7}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "55b9ec23-4e00-427b-b791-04a88b1327d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we start the next iteration #8 to minimize the loss\n",
    "losssum_7.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6ec5ae15-36c5-405a-b02a-c40a0a80d94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After backward#8, the data is\n",
      "Value(data=-0.8846515331221009, grad=-1.602359010806833, op=prim, label=L0|N0|w0)\n"
     ]
    }
   ],
   "source": [
    "print(f'After backward#8, the data is\\n{mlp_nn1.layers[0].neurons[0].w[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3fdea10e-3c47-4ab8-a6b4-2eadd2cb8ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we go to change the parameter slightly according to the gradient information to minimize the Loss function.\n",
    "# In a gradient descent scheme, we think of the Gradient Descent as a vector pointing in the direction of increased loss.\n",
    "# As we want to decrease the loss, we must go to change the parameter in the opposite direction.\n",
    "# So we increase the data of each parameter by multiplying with the negative 1% of it's grad\n",
    "for p in mlp_nn1.parameters():\n",
    "    p.data += -0.01 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "91fe6c0f-c82d-4aa6-b1f3-8f025263ac1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we are making a next forward pass ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9515750792350357, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=-0.9896797690734922, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=-0.9581596984928638, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=0.9354891324195409, grad=0, op=tanh, label=L2|N0|o)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Now we are making a next forward pass ...\")\n",
    "yspred_8 = [(mlp_nn1(x)) for x in xs]\n",
    "# Values labeled L2 | N0 | o are the ys predictions for each Example\n",
    "yspred_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "518cc225-ba3a-4d76-9bea-fcfae91c0164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... and recalculate the loss\n",
      "loss for all ground truths: Value(data=0.008363742983661115, grad=0, op=+, label=losssum)\n"
     ]
    }
   ],
   "source": [
    "print(\"... and recalculate the loss\")\n",
    "losss_8 = [(ypred - ygt)**2 for ygt, ypred in zip(ygts, yspred_8)]\n",
    "# label the loss by index for each loss\n",
    "for idx, loss in enumerate(losss_8): loss._label = f'loss{idx}'\n",
    "# The overall loss is\n",
    "losssum_8 = sum(losss_8)\n",
    "losssum_8._label = 'losssum'\n",
    "print(f'loss for all ground truths: {losssum_8}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0fa3d52d-abeb-4779-a755-84e985c5c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we start the next iteration #9 to minimize the loss\n",
    "losssum_8.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f6276921-2943-48a0-bd36-061be29dc11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After backward#9, the data is\n",
      "Value(data=-0.8686279430140326, grad=-1.606208092216953, op=prim, label=L0|N0|w0)\n"
     ]
    }
   ],
   "source": [
    "print(f'After backward#9, the data is\\n{mlp_nn1.layers[0].neurons[0].w[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2714a281-a47e-43e6-b4ac-896b376c71cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we go to change the parameter slightly according to the gradient information to minimize the Loss function.\n",
    "# In a gradient descent scheme, we think of the Gradient Descent as a vector pointing in the direction of increased loss.\n",
    "# As we want to decrease the loss, we must go to change the parameter in the opposite direction.\n",
    "# So we increase the data of each parameter by multiplying with the negative 1% of it's grad\n",
    "for p in mlp_nn1.parameters():\n",
    "    p.data += -0.01 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "91b324a2-33fb-4fde-b91e-9e0402da795f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we are making a next forward pass ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9627277755860726, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=-0.9918035519357604, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=-0.9597340937135229, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=0.9534033599201349, grad=0, op=tanh, label=L2|N0|o)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Now we are making a next forward pass ...\")\n",
    "yspred_9 = [(mlp_nn1(x)) for x in xs]\n",
    "# Values labeled L2 | N0 | o are the ys predictions for each Example\n",
    "yspred_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f3c6e5b1-25ca-48df-92ca-785386fbc8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... and recalculate the loss\n",
      "loss for all ground truths: Value(data=0.005248990549435787, grad=0, op=+, label=losssum)\n"
     ]
    }
   ],
   "source": [
    "print(\"... and recalculate the loss\")\n",
    "losss_9 = [(ypred - ygt)**2 for ygt, ypred in zip(ygts, yspred_9)]\n",
    "# label the loss by index for each loss\n",
    "for idx, loss in enumerate(losss_9): loss._label = f'loss{idx}'\n",
    "# The overall loss is\n",
    "losssum_9 = sum(losss_9)\n",
    "losssum_9._label = 'losssum'\n",
    "print(f'loss for all ground truths: {losssum_9}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
