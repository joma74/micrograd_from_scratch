{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7932766b-9a8d-466c-b779-916ae5064ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.wrighters.io/using-autoreload-to-speed-up-ipython-and-jupyter-work/\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# micograd_from_scratch_mlp_1, but by applying the modularized Classes from micrograd.nn\n",
    "from micrograd.tracegraph import draw_dot\n",
    "from micrograd.topo import build, findLeafNodes\n",
    "from micrograd.nn import Neuron, Layer, MLP\n",
    "from micrograd.engine import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e9a6196-ceb5-4a22-8bb0-29111ea24a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.8024582077087925, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=0.5413612523015063, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=0.35568112667755925, grad=0, op=tanh, label=L2|N0|o),\n",
       " Value(data=0.7577300512482477, grad=0, op=tanh, label=L2|N0|o)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset with 4 Input values, assigned each to each of the 3 Neurons of Layer 0.\n",
    "# The rows in the first column are the Input values assigned to each Neuron 0, 1, 2 and 3 of Layer 0.\n",
    "# The rows in the second column are the Input values assigned to each Neuron 0, 1, 2 and 3 of Layer 0.\n",
    "# The rows in the third column are the Input values assigned to each Neuron 0, 1, 2 and 3 of Layer 0.\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0], # Example #0 \n",
    "    [3.0, -1.0, 0.5], # Example #1\n",
    "    [0.5, 1.0, 1.0],  # Example #2\n",
    "    [1.0, 1.0, -1.0], # Example #3\n",
    "]\n",
    "# Desired targets for each Example #; a simple binary classifier; Also called the g(round) t(ruths)\n",
    "# Values labeled gt0, gt1, gt2 and gt3 are the ground truths for each Example\n",
    "ygts = [Value(1.0, _label='gt0'), Value(-1.0, _label='gt1'), Value(-1.0, _label='gt2'), Value(1.0, _label='gt3')]\n",
    "# A MLP neuronal network with 3 Input value and, 3 Layers by 4x4x1 Neurons\n",
    "mlp_nn1 = MLP(3, [4, 4, 1])\n",
    "# WANT\n",
    "# The current prediction for each Example\n",
    "#   The MLP to output  1.0 given Example #0\n",
    "#   The MLP to output -1.0 given Example #1\n",
    "#   The MLP to output -1.0 given Example #2\n",
    "#   The MLP to output  1.0 given Example #3\n",
    "yspred = [(mlp_nn1(x)) for x in xs]\n",
    "# Values labeled L2 | N0 | o are the ys predictions for each Example\n",
    "yspred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16854ec8-5f93-4981-ae17-b8b870abfa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for all ground truths: Value(data=4.311383315096003, grad=0, op=+, label=losssum)\n"
     ]
    }
   ],
   "source": [
    "# WANT\n",
    "# How do we tune the weights to better predict the desired targets?\n",
    "# Calculate a single number that measures the total performance of the neural net.\n",
    "# This single number is called the Loss. \n",
    "# So first, we implement the Loss function with a mean squared error Loss. The Loss is the difference between the \n",
    "# prediction and the gound truth of y. Squared, to always get a positive number.\n",
    "losss = [(ypred - ygt)**2 for ygt, ypred in zip(ygts, yspred)]\n",
    "# label the loss by index for each loss\n",
    "for idx, loss in enumerate(losss): loss._label = f'loss{idx}'\n",
    "# The overall loss is\n",
    "losssum = sum(losss)\n",
    "losssum._label = 'losssum'\n",
    "print(f'loss for all ground truths: {losssum}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef238017-14bd-4158-9adb-4b4b2304d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_dot of losssum before calling backward -> see doc/micograd_from_scratch_mlp_3_withlosssum_beforebackward.svg\n",
    "# draw_dot(losssum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2117e6-fc07-4a69-91a6-f06d01f85424",
   "metadata": {},
   "outputs": [],
   "source": [
    "losssum.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e06ea4c3-aaa9-47d4-9d63-95030d97ddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_dot of losssum after calling backward -> see doc/micograd_from_scratch_mlp_3_withlosssum_afterbackward.svg\n",
    "# draw_dot(losssum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cca1ab05-44ff-47d0-9f5f-fa51d5f4292b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we look at the Weight of some Neuron of some Layer: Value(data=-0.959487738115776, grad=-0.24558073915563292, op=prim, label=L0|N0|w0)\n",
      "... at the grad, we see that it's influence is ? positive or negative: -0.24558073915563292\n"
     ]
    }
   ],
   "source": [
    "# If we look at the 1st Weight of the 1st Neuron of the first Layer ...\n",
    "mlp_nn1.layers[0].neurons[0].w[0]\n",
    "# ... at the grad, we see that it's influence is positive\n",
    "print(f'If we look at the Weight of some Neuron of some Layer: {mlp_nn1.layers[0].neurons[0].w[0]}')\n",
    "print(f'... at the grad, we see that it\\'s influence is ? positive or negative: {mlp_nn1.layers[0].neurons[0].w[0].grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3084f71-0668-4e52-b83b-7948426f9ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters of the MLP:\n",
      "Value(data=-0.959487738115776, grad=-0.24558073915563292, op=prim, label=L0|N0|w0)\n",
      "Value(data=0.375987038868441, grad=-0.47746037458081275, op=prim, label=L0|N0|w1)\n",
      "Value(data=-0.41034046594409146, grad=-0.4603471973615333, op=prim, label=L0|N0|w2)\n",
      "Value(data=-0.5310559751256543, grad=-0.47132442367378147, op=prim, label=L0|N0|b)\n",
      "Value(data=-0.2587547667948096, grad=-11.328698606799321, op=prim, label=L0|N1|w0)\n",
      "Value(data=0.0403237811106445, grad=2.849281943171405, op=prim, label=L0|N1|w1)\n",
      "Value(data=0.5833304446497909, grad=-3.553646227793297, op=prim, label=L0|N1|w2)\n",
      "Value(data=0.4031334249509986, grad=-4.752577074474798, op=prim, label=L0|N1|b)\n",
      "Value(data=-0.08045905136012599, grad=0.5985595195990767, op=prim, label=L0|N2|w0)\n",
      "Value(data=0.8449829371178346, grad=-0.18795489475913127, op=prim, label=L0|N2|w1)\n",
      "Value(data=0.08749341362998475, grad=0.10041199251109707, op=prim, label=L0|N2|w2)\n",
      "Value(data=0.3596692071246599, grad=0.20629667644175093, op=prim, label=L0|N2|b)\n",
      "Value(data=0.7796002006479592, grad=-1.2415173401901167, op=prim, label=L0|N3|w0)\n",
      "Value(data=0.04629929934958121, grad=-1.5842420460481015, op=prim, label=L0|N3|w1)\n",
      "Value(data=-0.6117195051213022, grad=-1.8156334008752415, op=prim, label=L0|N3|w2)\n",
      "Value(data=-0.22379499140156267, grad=-1.8490957812499142, op=prim, label=L0|N3|b)\n",
      "Value(data=0.5919572162407445, grad=2.6655881355656974, op=prim, label=L1|N0|w0)\n",
      "Value(data=0.7502516922776772, grad=-0.7222992329664458, op=prim, label=L1|N0|w1)\n",
      "Value(data=-0.6534884560395511, grad=0.019621007775612112, op=prim, label=L1|N0|w2)\n",
      "Value(data=0.15380220502080544, grad=-1.0893193840900592, op=prim, label=L1|N0|w3)\n",
      "Value(data=-0.2328772444323277, grad=-2.9175329924542477, op=prim, label=L1|N0|b)\n",
      "Value(data=-0.12066568983712389, grad=-2.7278817393329047, op=prim, label=L1|N1|w0)\n",
      "Value(data=-0.8987506612414735, grad=1.1114592993813779, op=prim, label=L1|N1|w1)\n",
      "Value(data=-0.1225969681040342, grad=0.28475382209427597, op=prim, label=L1|N1|w2)\n",
      "Value(data=-0.7520032732538868, grad=0.6381276471785513, op=prim, label=L1|N1|w3)\n",
      "Value(data=0.6533380741557036, grad=3.016088851186503, op=prim, label=L1|N1|b)\n",
      "Value(data=-0.618663543720062, grad=1.054383458376593, op=prim, label=L1|N2|w0)\n",
      "Value(data=0.6506394867425636, grad=-0.028228791546522865, op=prim, label=L1|N2|w1)\n",
      "Value(data=-0.019294303619492137, grad=0.6815316267310784, op=prim, label=L1|N2|w2)\n",
      "Value(data=-0.9494788848380327, grad=-0.8160741463350675, op=prim, label=L1|N2|w3)\n",
      "Value(data=0.537267865322975, grad=-1.0223003498168848, op=prim, label=L1|N2|b)\n",
      "Value(data=-0.3284652770631633, grad=-1.7297032682090074, op=prim, label=L1|N3|w0)\n",
      "Value(data=-0.32139556354765086, grad=0.5823595342024817, op=prim, label=L1|N3|w1)\n",
      "Value(data=-0.6331299279255371, grad=0.13549746095349718, op=prim, label=L1|N3|w2)\n",
      "Value(data=-0.7200097414668492, grad=0.5547449138344112, op=prim, label=L1|N3|w3)\n",
      "Value(data=-0.20094822814925162, grad=1.9157080532314734, op=prim, label=L1|N3|b)\n",
      "Value(data=-0.9164493483231038, grad=-2.025157537341182, op=prim, label=L2|N0|w0)\n",
      "Value(data=0.7677588618193489, grad=1.0702685782554813, op=prim, label=L2|N0|w1)\n",
      "Value(data=-0.510885440797914, grad=2.7343297817332375, op=prim, label=L2|N0|w2)\n",
      "Value(data=0.48313247676857607, grad=-1.005128353278395, op=prim, label=L2|N0|w3)\n",
      "Value(data=0.24915115041220282, grad=4.200600428883557, op=prim, label=L2|N0|b)\n",
      "\n",
      "Number of Parameters of the MLP: 41\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# If the Grad of this Weight of this particular Neuron of this particular Layer is positive, the influence of the Weight to \n",
    "# the Loss is also positive. So decreasing the Weight of this particular Neuron would make the loss go down.\n",
    "#\n",
    "# If the Grad of this Weight of this particular Neuron of this particular Layer is negative, the influence of the Weight to\n",
    "# the Loss is also negative. So increasing the Weight of this particular Neuron would make the loss go down.\n",
    "#\n",
    "# WANT\n",
    "# A convenience methode, to gather all those parameters called Weights and Biases so we can change them.\n",
    "from io import StringIO\n",
    "output = StringIO()\n",
    "print(*mlp_nn1.parameters(), sep=\"\\n\", file=output)\n",
    "print(f'Parameters of the MLP:\\n{output.getvalue()}')\n",
    "print(f'Number of Parameters of the MLP: {len(mlp_nn1.parameters())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c6200f-24df-4d2f-a228-0c0a2f930f70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
